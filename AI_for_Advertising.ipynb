{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo - http://www.ai-writer.com/?page=view_result&type=article&id=14300&pass=7JClv4aayH\n",
    "\n",
    "### 마켓팅에서의 AI 실제 사례 \n",
    "\n",
    "![alt text](https://whatsthebigdata.files.wordpress.com/2017/02/ai_marketing.jpg?w=640 \"Logo Title Text 1\")\n",
    "\n",
    "- 잠재고객 타겟팅 (데이터를 분석하고 고객들을 점수화 합니다. 이는 고객들이 물건 구매나 영상 시청 등의 액션을 취할 것 같은 지에 근거합니다.)\n",
    "- 콘텐츠 제작 (어떤 종류의 콘텐츠가 고객들을 가장 잘 구매전환 시킬지)\n",
    "- 실시간 최적화 (어떻게 해야 콘텐츠를 고객에게 가장 잘 맞도록 실시간으로 최적화할지)\n",
    "- 기계들은 수년간 자동 콘텐츠 제작에 이용되어 왔습니다.특히 AP연합통신, 야후, 폭스와 같은 회사들이 꽤 오래 사용해오고 있죠.\n",
    "\n",
    "![alt text](https://blog.getresponse.com/uploads/2017/03/demandbasechart.png \"Logo Title Text 1\")\n",
    "\n",
    "### 이 분야의 스타트업들\n",
    "\n",
    "#### Appier\n",
    "\n",
    "![alt text](http://www.mobyaffiliates.com/wp-content/uploads/2015/11/appier.png \"Logo Title Text 1\")\n",
    "\n",
    "- Appier는 설립 5년차에 누적 약 500억 원 이상의 투자를 유치하였습니다.\n",
    "- 이 회사는 AI를 사용하여 잠재 고객들이 다음에 무엇을 할 지를 예측합니다.\n",
    "- 이 회사의 실시간 광고 최적화 엔진은 여러 개의 디지털 기기 간, 특히 스마트폰을 중심으로 잠재 고객들을 찾아냅니다.\n",
    "- 잠재고객 데이터 풀을 더 진보되고 전문적인 버전으로 만들어주는데, 이는 과거에는 ‘cookie pools’이라고 불렸습니다.\n",
    "- 만일 당신의 고객들이 단지 신발만 구입하고자 했다면, 당장은 다른 신발을 더 사진 않을 지 몰라도 양말은 살 수 있지 않을까요? Appier에 가입해보세요\n",
    "\n",
    "#### Drawbridge\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/ai100-170927214644/95/artificial-intelligence-ai-100-startups-2017-75-638.jpg?cb=1510037429 \"Logo Title Text 1\")\n",
    "\n",
    "- Drawbridge는 cross-device 도달율에 관한 모든 것을 다루며, 특허를 가지고 있습니다. (on a \"장치 사용에 기반한 인터넷장치 그룹핑 시스템\")\n",
    "- 약 500억 원 가량을 투자 받았습니다.\n",
    "- \"익명의 ID\"를 만드는데 이것은 광고뿐 아니라 더 많은 곳에 사용될 수 있습니다.(사기 탐지 등 말이죠)\n",
    "- 그들은 사람들이 언제 장비를 바꿔 사용할 지를 알 수 있으며, 광고주들은 그 장비간 이동 타이밍을 자본화할 수 있습니다.\n",
    "\n",
    "####  Insidesales.com \n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/il0wblpdsxgqdkrqwonw-signature-d9877cf01f81af06f005d08076d17a65d7a03928ceb1de73653a982ec29c2710-poli-170316163616/95/michael-plante-inside-sales-the-ai-revolution-6-638.jpg?cb=1490208673 \"Logo Title Text 1\")\n",
    "\n",
    "- 약 3천억원을 투자 받았으며, 2004년도에 설립되었습니다.\n",
    "- 당신이 큰 잠재적인 매출을 기대하고 있다면, 고객들 전부에게 한 번에 집중하려 했다간 아무것도 얻지 못 할 것입니다.\n",
    "- 이 회사는 구매전환 가능성이 가장 높은 상위 20%의 고객들을 예측할 수 있게 도와줍니다.\n",
    "\n",
    "#### Persado\n",
    "\n",
    "![alt text](https://beta.techcrunch.com/wp-content/uploads/2013/02/screen-shot-2013-02-13-at-11-43-05.png \"Logo Title Text 1\")\n",
    "\n",
    "- 이 회사는 문구와 단어들을 찾아내어, 잠재고객들에게 행동 유발을 극대화합니다.\n",
    "- 어떤 종류의 광고가 잠재고객들의 행동 유발을 극대화할 수 있을까요? 감성과 연관성이라는 요소를 발전시켜 나온 무언가 일 것입니다.\n",
    "- 이 회사의 솔루션은 마켓팅 funnel 단계 상 가장 첫 진입점에 관한 기술이라, 인수될 가능성이 있습니다. \n",
    "\n",
    "### Audience Targeting\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*Zhm1NMlmVywn0G18w3exog.png \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://recsysjd.files.wordpress.com/2016/09/ss.png?w=368&h=226 \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://i.imgur.com/bmW79NS.png \"Logo Title Text 1\")\n",
    "\n",
    "- 많이들 사용하는 LightFM 라이브러리에서는, 사용자-아이템 관계를 인수분해 기계 기능과 각 피처의 선형 임베딩을 학습하여 표현할 수 있습니다.\n",
    "- 이 두개의 representation 벡터를 곱하면 단위가 없는 점수가 나오는데\n",
    "- 이 점수로 순위를 매기면 주어진 사용자-아이템의 조합이 얼마나 좋은지 (또는 나쁜지)를 알수 있습니다.\n",
    "- 선형 인수분해 방법은 효과적이며 연산효율도 높습니다.\n",
    "- 하지만 심층 신경망은 더욱 의미있는 표상을 만들어내도록 개선될 수 있습니다.\n",
    "- TensorRec은 텐서플로우에서 돌아가는 최신 라이브러리로, 이 문제를 풀기 위해 개발되었습니다.\n",
    "- TensorRec은 학습과 예측에 필요한 쉬운 API를 제공하는 추천 알고리즘이며, 파이썬의 일반적인 머신러닝 툴들과 유사한 형태를 가지고 있습니다.\n",
    "\n",
    "![alt text](https://www.altoros.com/blog/wp-content/uploads/2018/03/multilayer-perceptron-with-tensorflow-architecture-for-recommender-systems-v1.png \"Logo Title Text 1\")\n",
    "\n",
    "- 여러분 각자의 표상과 손실 함수로도 실험해볼 수 있도록 유연하기 때문에, 여러분들은 이것으로 특정한 사용자-아이템 표상을 위한 전용 추천 시스템을 만들 수 있습니다.\n",
    "- 이 추천 엔진은, 직접적인 긍정과 부정 피드백으로부터 학습할 수도 있습니다.\n",
    "- 이 엔진은, 임의의 텐서플로우 그래프들도 representation 함수와,손실함수로써 사용될 수 있도록 해줍니다.\n",
    "- 또한 representation 함수와, 손실함수를 위한 합리적인 기본값도 제공해줍니다.\n",
    "- 이것으로 추천엔진의 스코어를 정할 수 있는데, 여기에 사용자-아이템의 특징들(아이디, 태그와 기타 메타데이타들)과 두개의 저차원 벡터 (\"user representation\"과 \"item representation\")를 사용합니다.\n",
    "- 이 두 벡터의 내적이 사용자와, 사용자-아이템 간 관계의 점수입니다. 가장 높은 점수를 최고의 추천이라고 예측합니다.\n",
    "- 이 표상을 생성하는 알고리즘을 representation function이라 부르며, 커스터마이즈할 수 있습니다: 똑바로 된 직선 변환에서부터 심층 신경망까지 어떤 것이라도 적용될 수 있습니다.\n",
    "- 이것은 생성해낸 스코어를 사용자와 아이템간 사이의 실제 피드백(좋고/싫고)와 비교하면서 학습합니다.\n",
    "- 이러한 비교자(comparator)를 \"손실함수\"라 하며, TensorRec은 여러분 자신만의 손실함수를 사용할 수 있도록 커스타마이즈를 지원합니다.\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/jfkirk/tensorrec/master/examples/system_diagram.png \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "#### 4 Steps!\n",
    "\n",
    "1. 임베딩 편의를 위해 입력 데이타를 feature 텐서로 변환합니다.\n",
    "2. 사용자/아이템 feature 텐서들을 사용자/아이템 representation로 변환합니다. (representation 함수).\n",
    "3. representations 쌍을 예측값으로 변환합니다.\n",
    "4. 예측값과 진리값을 손실값으로 변환합니다. (손실함수)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorrec\n",
    "\n",
    "# 기본 파라메터로 모델을 빌드합니다.\n",
    "model = tensorrec.TensorRec()\n",
    "\n",
    "# 더미데이터를 생성합니다.\n",
    "interactions, user_features, item_features = tensorrec.util.generate_dummy_data(\n",
    "    num_users=100,\n",
    "    num_items=150,\n",
    "    interaction_density=.05\n",
    ")\n",
    "\n",
    "# 모델을 5 epochs 만큼 fit합니다.\n",
    "model.fit(interactions, user_features, item_features, epochs=5, verbose=True)\n",
    "\n",
    "# 모든 사용자와 모든 아이템들에 대해 점수를 예측합니다.\n",
    "predictions = model.predict(user_features=user_features,\n",
    "                            item_features=item_features)\n",
    "\n",
    "# 10에서 recall을 계산하고 출력합니다.\n",
    "r_at_k = tensorrec.eval.recall_at_k(model, interactions,\n",
    "                                    k=10,\n",
    "                                    user_features=user_features,\n",
    "                                    item_features=item_features)\n",
    "print(np.mean(r_at_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 콘텐츠 제작\n",
    "\n",
    "![alt text](https://www.kdnuggets.com/wp-content/uploads/RNN.jpg \"Logo Title Text 1\")\n",
    "\n",
    "http://www.ai-writer.com/?page=view_result&type=article&id=14300&pass=7JClv4aayH\n",
    "\n",
    "![alt text](http://keras.io/img/regular_stacked_lstm.png \"Logo Title Text 1\")\n",
    "\n",
    "- AI 마케터들은 자동으로 콘텐츠를 만들 수 있습니다. 주식 시황이나 스포츠 리포트 같이 단순한 이야기 같은 것들이요. \n",
    "- 여러분들은 아마도 이미 알아채지도 못한채 알고리즘이 쓴 콘텐츠를 읽어보셨을 겁니다.\n",
    "- 다음 시작 부분이 온전히 알고리즘에 의해서만 작성되었다는 사실을 알면 좀 놀라실 수도 있을 것 같아요\n",
    "\n",
    "#### \"Tuesday was a great day for W. Roberts, as the junior pitcher threw a perfect game to carry Virginia to a 2-0 victory over  at Davenport Field.\"\n",
    "#### \"지난 화요일은 W. Roberts 최고의 날이었습니다. Davenport Field에서 열린 George Washington과의 경기에서, Virginia의 2-0 승리를 이끈 퍼펙트 게임의 주인공이었습니다.\"\n",
    "\n",
    "- 이미지, 비디오? GAN을 쓰세요 \n",
    "- 오디오? WaveNet을 쓰세요\n",
    "- 텍스트? LSTM RNN을 쓰세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''니체의 글로부터 텍스트를 생성하는 예제입니다.\n",
    "일관성을 갖춘 생성된 텍스트를 생성하려면, 최소 20 epochs를 돌려야합니다.\n",
    "GPU로 돌리길 추천합니다\n",
    "RNN은 꽤 연산이 많기 때문이죠\n",
    "만일 이 스크립트를 새로운 데이타로 돌리고자 한다면\n",
    "말뭉치가 최소  ~100k개, ~1M개면 더 좋습니다.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "\n",
    "path = get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "with io.open(path, encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "\n",
    "# 모델을 빌드합니다: 1개의 LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # 확률 배열로부터 샘플링하기 위한 헬퍼함수 입니다.\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # 매번 epoch 끝에서 불리는 함수. 생성된 텍스트를 출력합니다\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=60,\n",
    "          callbacks=[print_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
