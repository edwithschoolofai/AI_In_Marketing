{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo - http://www.ai-writer.com/?page=view_result&type=article&id=14300&pass=7JClv4aayH\n",
    "\n",
    "### 마켓팅에서의 AI 실제 사례 \n",
    "\n",
    "![alt text](https://whatsthebigdata.files.wordpress.com/2017/02/ai_marketing.jpg?w=640 \"Logo Title Text 1\")\n",
    "\n",
    "- 잠재고객 타겟팅 (데이터를 분석하고 고객들을 점수화 합니다. 이는 고객들이 물건 구매나 영상 시청 등의 액션을 취할 것 같은 지에 근거합니다.) \n",
    "- 콘텐츠 제작 (어떤 종류의 콘텐츠가 고객들을 가장 잘 구매전환 시킬지)\n",
    "- 실시간 최적화 (어떻게 해야 콘텐츠를 고객에게 가장 잘 맞도록 실시간으로 최적화할지)\n",
    "- 기계들은 수년간 자동 콘텐츠 제작에 이용되어 왔습니다.특히 AP연합통신, 야후, 폭스와 같은 회사들이 꽤 오래 사용해오고 있죠.\n",
    "\n",
    "![alt text](https://blog.getresponse.com/uploads/2017/03/demandbasechart.png \"Logo Title Text 1\")\n",
    "\n",
    "### 이 분야의 스타트업들\n",
    "\n",
    "#### Appier\n",
    "\n",
    "![alt text](http://www.mobyaffiliates.com/wp-content/uploads/2015/11/appier.png \"Logo Title Text 1\")\n",
    "\n",
    "- Appier는 설립 5년차에 누적 약 500억 원 이상의 투자를 유치하였습니다. \n",
    "- 이 회사는 AI를 사용하여 잠재 고객들이 다음에 무엇을 할 지를 예측합니다.\n",
    "- 이 회사의 실시간 광고 최적화 엔진은 여러 개의 디지털 기기 간, 특히 스마트폰을 중심으로 잠재 고객들을 찾아냅니다.. \n",
    "- 잠재고객 데이터 풀을 더 진보되고 전문적인 버전으로 만들어주는데, 이는 과거에는 ‘cookie pools’이라고 불렸습니다.\n",
    "- 만일 당신의 고객들이 단지 신발만 구입하고자 했다면, 당장은 다른 신발을 더 사진 않을 지 몰라도 양말은 살 수 있지 않을까요? Appier에 가입해보세요\n",
    "\n",
    "#### Drawbridge\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/ai100-170927214644/95/artificial-intelligence-ai-100-startups-2017-75-638.jpg?cb=1510037429 \"Logo Title Text 1\")\n",
    "\n",
    "- Drawbridge는 cross-device 도달율에 관한 모든 것을 다루며, 특허를 가지고 있습니다. (on a “장치 사용에 기반한 인터넷장치 그룹핑 시스템”). \n",
    "- 약 500억 원 가량을 투자 받았습니다.\n",
    "- “익명의 ID”를 만드는데 이것은 광고뿐 아니라 더 많은 곳에 사용될 수 있습니다.(사기 탐지 등 말이죠)\n",
    "- 그들은 사람들이 언제 장비를 바꿔 사용할 지를 알 수 있으며, 광고주들은 그 장비간 이동 타이밍을 자본화할 수 있습니다.\n",
    "\n",
    "####  Insidesales.com \n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/il0wblpdsxgqdkrqwonw-signature-d9877cf01f81af06f005d08076d17a65d7a03928ceb1de73653a982ec29c2710-poli-170316163616/95/michael-plante-inside-sales-the-ai-revolution-6-638.jpg?cb=1490208673 \"Logo Title Text 1\")\n",
    "\n",
    "- 약 3천억원을 투자 받았으며, 2004년도에 설립되었습니다.\n",
    "- 당신이 큰 잠재적인 매출을 기대하고 있다면, 고객들 전부에게 한 번에 집중하려 했다간 아무것도 얻지 못 할 것입니다.\n",
    "- 이 회사는 구매전환 가능성이 가장 높은 상위 20%의 고객들을 예측할 수 있게 도와줍니다.\n",
    "\n",
    "#### Persado\n",
    "\n",
    "![alt text](https://beta.techcrunch.com/wp-content/uploads/2013/02/screen-shot-2013-02-13-at-11-43-05.png \"Logo Title Text 1\")\n",
    "\n",
    "- They find the phrases and words that drive the greatest action for your audience. \n",
    "- What kind of ad will drive the greatest action for your audience? Build off of emotions and relevance.\n",
    "- They are at thetop of the funnel, driving acquisitions. \n",
    "\n",
    "### Audience Targeting\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*Zhm1NMlmVywn0G18w3exog.png \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://recsysjd.files.wordpress.com/2016/09/ss.png?w=368&h=226 \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://i.imgur.com/bmW79NS.png \"Logo Title Text 1\")\n",
    "\n",
    "- In the popular LightFM library, it generates user and item representations by functioning as a factorization machine and learning linear embeddings for each feature. \n",
    "- It takes the dot product of these two representation vectors and gets a unitless score that\n",
    "- when ranked, the score tells you how good (or bad) a given user-item match would be.\n",
    "- linear factorization method is effective and computationally efficient\n",
    "- But deep neural networks could improve on this by creating more meaningful representations \n",
    "- TensorRec is a newer library built on top of tensorflow that aims to solve that\n",
    "- TensorRec is a recommendation algorithm with an easy API for training and prediction that resembles common machine learning tools in Python. \n",
    "\n",
    "![alt text](https://www.altoros.com/blog/wp-content/uploads/2018/03/multilayer-perceptron-with-tensorflow-architecture-for-recommender-systems-v1.png \"Logo Title Text 1\")\n",
    "\n",
    "- It also gives you the flexibility to experiment with your own representation and loss functions, letting you build a recommendation system that is tailored to understanding your particular users and items.\n",
    "- It is a recommendation engine capable of learning from explicit positive and negative feedback.\n",
    "- It allows for arbitrary TensorFlow graphs to be used as representation functions and loss functions.\n",
    "- And it provides reasonable defaults for representation functions and loss functions.\n",
    "- It scores recommendations by consuming user and item features (ids, tags, or other metadata) and building two low-dimensional vectors, a “user representation” and an “item representation”. \n",
    "- The dot product of these two vectors is the score for the relationship between that user and that item — the highest scores are predicted to be the best recommendations.\n",
    "-  The algorithm used to generate these representations, called the representation function, can be customized: anything from a straight-forward linear transform to a deep neural network can be applied\n",
    "-  It learns by comparing the scores it generates to actual interactions (likes/dislikes) between users and items. \n",
    "- The comparator is called the “loss function,” and TensorRec allows you to customize your own loss functions as well.\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/jfkirk/tensorrec/master/examples/system_diagram.png \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "#### 4 Steps!\n",
    "\n",
    "1. Transform input data into feature tensors for easy embedding.\n",
    "2. Transform user/item feature tensors into user/item representations (the representation function).\n",
    "3. Transform a pair of representations into a prediction.\n",
    "4. Transform predictions and truth values into a loss value (the loss function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorrec\n",
    "\n",
    "# Build the model with default parameters\n",
    "model = tensorrec.TensorRec()\n",
    "\n",
    "# Generate some dummy data\n",
    "interactions, user_features, item_features = tensorrec.util.generate_dummy_data(\n",
    "    num_users=100,\n",
    "    num_items=150,\n",
    "    interaction_density=.05\n",
    ")\n",
    "\n",
    "# Fit the model for 5 epochs\n",
    "model.fit(interactions, user_features, item_features, epochs=5, verbose=True)\n",
    "\n",
    "# Predict scores for all users and all items\n",
    "predictions = model.predict(user_features=user_features,\n",
    "                            item_features=item_features)\n",
    "\n",
    "# Calculate and print the recall at 10\n",
    "r_at_k = tensorrec.eval.recall_at_k(model, interactions,\n",
    "                                    k=10,\n",
    "                                    user_features=user_features,\n",
    "                                    item_features=item_features)\n",
    "print(np.mean(r_at_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Creation\n",
    "\n",
    "![alt text](https://www.kdnuggets.com/wp-content/uploads/RNN.jpg \"Logo Title Text 1\")\n",
    "\n",
    "http://www.ai-writer.com/?page=view_result&type=article&id=14300&pass=7JClv4aayH\n",
    "\n",
    "![alt text](http://keras.io/img/regular_stacked_lstm.png \"Logo Title Text 1\")\n",
    "\n",
    "- With AI marketers can automatically generate content for simple stories such as stock updates and sports reports. \n",
    "- You’ve probably even read content written by an algorithm without noticing it!\n",
    "- It may surprise you that the following opening sentence is a sports story written solely by an algorithm:\n",
    "\n",
    "#### “Tuesday was a great day for W. Roberts, as the junior pitcher threw a perfect game to carry Virginia to a 2-0 victory over George Washington at Davenport Field.”\n",
    "\n",
    "- Images, video? Use Generative Adversarial networks \n",
    "- Audio? Use WaveNet\n",
    "- Text? Use LSTM Recurrent Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Example script to generate text from Nietzsche's writings.\n",
    "At least 20 epochs are required before the generated text\n",
    "starts sounding coherent.\n",
    "It is recommended to run this script on GPU, as recurrent\n",
    "networks are quite computationally intensive.\n",
    "If you try this script on new data, make sure your corpus\n",
    "has at least ~100k characters. ~1M is better.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "\n",
    "path = get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "with io.open(path, encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "\n",
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=60,\n",
    "          callbacks=[print_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
